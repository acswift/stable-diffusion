{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Verdana;}
{\colortbl;\red255\green255\blue255;\red110\green5\blue2;}
{\*\expandedcolortbl;;\cssrgb\c51239\c6511\c0;}
\paperw11900\paperh16840\margl1138\margr1138\margb562\margt562\vieww12200\viewh17140\viewkind1
\deftab720
\pard\pardeftab720\ri-6\sl288\slmult1\partightenfactor0

\f0\fs20 \cf2 \
\
\
on intel: https://github.com/viking1304/a1111-setup\
\
\
\
\
\
settings \'9b stable diffusion \'9b stable diffusion \'9b \uc0\u8730  Upcast cross attention layer to float32\
\
shaved 45 seconds\
\
\
\pard\pardeftab720\ri-6\sl288\slmult1\sb280\sa280\partightenfactor0
\cf2 research\
\pard\pardeftab720\ri-6\sl288\slmult1\partightenfactor0
\cf2 \
https://huggingface.co/docs/diffusers/en/optimization/mps\
https://pytorch.org/get-started/locally/\
\
\
\pard\pardeftab720\ri-6\sl288\slmult1\sb280\sa280\partightenfactor0
\cf2 versions\
\pard\pardeftab720\ri-6\sl288\slmult1\partightenfactor0
\cf2 python3 --version\
Python 3.12.2\
\
python3 -m pip --version\
pip 24.0 from /opt/homebrew/lib/python3.12/site-packages/pip (python 3.12)\
\
conda list anaconda$\
-bash: conda: command not found\
\
python3\
Python 3.12.2 (main, Feb  6 2024, 20:19:44) [Clang 15.0.0 (clang-1500.1.0.2.5)] on darwin\
Type "help", "copyright", "credits" or "license" for more information.\
>>> import torch;\
Traceback (most recent call last):\
  File "<stdin>", line 1, in <module>\
ModuleNotFoundError: No module named 'torch'\
>>> torch.__version__\
\
\
\
https://www.reddit.com/r/StableDiffusion/comments/1c0w01f/seeking_macbook_pro_automatic1111_optimized_setup/\
\
UPDATE: Working MUCH faster now thanks to the below #COMMANLINE_ARGS="" adjustment to the /stable-diffusion-webui/webui-user.sh\
\
export COMMANDLINE_ARGS="--skip-torch-cuda-test --upcast-sampling --opt-split-attention"\
\
Additionally launching with PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 ./webui.sh \
\
AND Enabling Upcast cross attention layer to float32 under settings.\
\
\
\
https://github.com/lllyasviel/Fooocus/discussions/1156\
\
Edit the launch.py and add: os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"\
\
it's a system variable, used before launching SD\
so I added it to my .bashrc\
\
\
\
https://www.reddit.com/r/StableDiffusion/comments/183bxod/how_to_increase_speed_of_autom1111_on_m1_mac/\
\
Also add these in webui-user.sh:\
\
export COMMANDLINE_ARGS="--opt-split-attention-v1 --no-half --skip-torch-cuda-test\
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\
export PYTORCH_ENABLE_MPS_FALLBACK=1\
\
\
\
Use CoreML: No\
and this was ~68s. CoreML speeds up your render time, but increases the loading time (by a lot). If you switch models with any sort of frequency it's not worth it, but if you know you're going to use the same model for a while it might be worth turning on.\
\
\
}